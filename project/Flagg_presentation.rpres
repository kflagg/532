On the Importance of Promoting Bayesian Statistics
========================================================
author: Kenny Flagg
date: December 4, 2015

```{r setup, include=FALSE}
opts_chunk$set(cache = TRUE, scipen = 5)
require(R2jags)
```

The Situation: Modeling the Oil Field Discovery Process
========================================================

![Map of Halten Terrace](haltenmap.png)

***

Halten Terrace, Offshore Norway
* 22 fields discovered
* Questions of interest:
  + How many undiscovered fields are present?
  + What is the total volume of oil in the undiscovered fields?

The Discovery Process Model
========================================================

* Discovered field sizes: $Y_1,\dots,Y_n$
* Undiscovered field sizes: $S_{n+1},\dots,S_N$
* $N$ unknown
* The $Y_j$ and $S_k$ come from a $\mathrm{lognormal}(\mu,\sigma^2)$
population
* $(Y_1,\dots,Y_n)$ are a random sample from the population
* $Y_j$ sampled with probability proportional to $Y_j^\beta$
* $\mu$, $\sigma^2$, $\beta$ all unknown

The Discovery Process Model
========================================================

Likelihood:

$$p(\mathbf{Y}|N,\beta,\mu,\sigma^2,\mathbf{S})
=\begin{pmatrix}N \\ n\end{pmatrix}\prod_{i=1}^n\left(\frac{Y_i^\beta
\mathrm{logN}(Y_i|\mu,\sigma^2)}{\sum_{j=i}^nY_j+\sum_{k=n+1}^NS_k}\right)$$

* Sinding-Larsen and Chen (1996) fit by maximum likelihood
* Difficulties in finding estimates motivated the empirical Bayes
approach of Sinding-Larsen and Xu (2005)

Monte Carlo Method of Xu and Sinding-Larsen
========================================================

1. Draw $\left(N^{(i)},\beta^{(i)},\mu^{(i)},\sigma^{2(i)}\right)$
from the prior
2. Draw $\left(S_{n+1}^{(i)},\dots,S_N^{(i)}\right)$ independently
from $\mathrm{lognormal}\left(\mu^{(i)},\sigma^{2(i)}\right)$
3. Compute the likelihood weights $W_i=p\left(\mathbf{Y}|N^{(i)},
\beta^{(i)},\mu^{(i)},\sigma^{2(i)},\mathbf{S}^{(i)}\right)$

Posterior expectations and probabilities can be computed as weighted sums.

But I want POSTERIOR DRAWS!
========================================================

If the prior is used as a proposal distribution for rejection sampling,
the normalized likelihood is the acceptance probability.

I proposed 500,000 draws and 84 were accepted.

***

```{r reject, echo=FALSE}
halten <- data.frame('Name' = c('Midtgard',
                                 'Tyrihans S',
                                 'Tyrihans N',
                                 'Smoerbukk',
                                 'Heidrun',
                                 'Smoebukk S',
                                 'Njord',
                                 'Mikkel',
                                 'Trestakk',
                                 'Alve',
                                 '6507/8-4',
                                 '6407/8-2',
                                 'Lavrans',
                                 'Kristin',
                                 'Ragnfrid',
                                 'Sharv',
                                 'Erled',
                                 '6407/9-9',
                                 '6507/7-6',
                                 '6407/7-6',
                                 '6506/6-1',
                                 'Svale'),
                     'Order' = 1:22,
                     'Size00' = c(113.20,
                                   22.6,
                                   26.3,
                                  158.5,
                                  125.1,
                                   67.3,
                                   42.2,
                                   33.63,
                                    7,
                                   18.5,
                                    3,
                                    1.8,
                                   44.2,
                                   92,
                                   10.7,
                                   60.27,
                                   10.4,
                                    2.1,
                                   18,
                                    2,
                                   93.2,
                                   10.1),
                     'Size92' = c(101,
                                   15.5,
                                   18.9,
                                  125.3,
                                  109.4,
                                   49.7,
                                   36,
                                   21.1,
                                    3.9,
                                      NA,
                                   20.4,
                                      NA,
                                      NA,
                                      NA,
                                      NA,
                                      NA,
                                      NA,
                                      NA,
                                      NA,
                                      NA,
                                      NA,
                                      NA),
                     'Zone' = c(1,
                                1,
                                1,
                                1,
                                1,
                                1,
                                1,
                                1,
                                1,
                                2,
                                1,
                                1,
                                1,
                                2,
                                2,
                                2,
                                2,
                                1,
                                2,
                                1,
                                2,
                                2))
M.prior <- function(M, n){
  return(ifelse(M-n<13, 0,
         ifelse(M-n<16, 0.05/3,
         ifelse(M-n<20, 0.2/4,
         ifelse(M-n<26, 0.25/6,
         ifelse(M-n<30, 0.25/4,
         ifelse(M-n<35, 0.2/5,
         ifelse(M-n<38, 0.05/3,
         0))))))))
}
ldlogn <- function(y, mu, sigma){
  return(dnorm(log(y), mu, sigma, log = TRUE) - log(y))
}
loglik <- function(Y, S, mu, sigma, beta, N, n){
  return(
    lchoose(N, n) +
      beta * sum(log(Y)) -
      sum(log(
        sapply(1:n, function(i){return(sum(Y[i:n] ^ beta))}) + sum(S ^ beta)
        )) +
      sum(ldlogn(Y, mu, sigma))
    )
}
set.seed(87624) # Don't use 87625!!!
num.draws1 <- 500000

# Create a data frame to store all the prior stuff.
draws1 <- data.frame('mu' = numeric(num.draws1),
                     'sigsq' = numeric(num.draws1),
                     'b0' = numeric(num.draws1),
                     'b' = numeric(num.draws1),
                     'beta' = numeric(num.draws1),
                     'M' = numeric(num.draws1),
                     'p' = numeric(num.draws1),
                     'N' = numeric(num.draws1),
                     'maxS' = numeric(num.draws1),
                     'totS' = numeric(num.draws1),
                     'lW' = numeric(num.draws1),
                     'W' = numeric(num.draws1),
                     'accept' = logical(num.draws1))

# Define number found so far.
n.found1 <- nrow(halten)

# Draw mu and sigmasq.
draws1$mu <- rnorm(num.draws1, 2.38, sqrt(0.54))
draws1$sigsq <- rgamma(num.draws1, 2.89^2/2.13, 2.89/2.13)

# Draw b0.
draws1$b0 <- runif(num.draws1, -0.213, -0.088)

# Draw b from the asymptotic sampling distribution of the slope of
# the simple linear regression of log(size) against the  discovery order.
order1.lm <- lm(log(Size00) ~ Order, data = halten)
bhat1 <- coef(order1.lm)[2]
se.bhat1 <- arm::se.coef(order1.lm)[2]
draws1$b <- rnorm(num.draws1, bhat1, se.bhat1)

# Discard impossible draws of b and b0 and get new ones.
while(sum(draws1$b<draws1$b0)>0){
  idx <- draws1$b<draws1$b0
  num.newdraws <- sum(idx)
  draws1$b0[idx] <- runif(num.newdraws, -0.213, -0.088)
  draws1$b[idx] <- rnorm(num.newdraws, bhat1, se.bhat1)
}

# Compute beta.
draws1$beta <- -log(1-draws1$b/draws1$b0)/sd(log(halten$Size00))

# Draw M and pi.
draws1$M <- sample(35:59, size = num.draws1, replace = TRUE,
                   prob = M.prior(35:59, n.found1))
draws1$p <- rbeta(num.draws1, 2.3, 4.28)

# Draw N.
draws1$N <- n.found1 + rbinom(num.draws1, draws1$M-n.found1, draws1$p)

# Draw S.
S.draws1 <- apply(draws1, 1, function(x){
    return(exp(rnorm(x['N']-n.found1, x['mu'], sqrt(x['sigsq']))))
  })

# Compute size of largest undiscovered field.
draws1$maxS <- sapply(S.draws1, function(x){return(
    ifelse(length(x)>0, max(x), NA)
  )})

# Compute total undiscovered volume.
draws1$totS <- sapply(S.draws1, function(x){return(
    ifelse(length(x)>0, sum(x), NA)
  )})

# Weights.
draws1$lW <- sapply(1:num.draws1,function(x){return(with(draws1,
    loglik(halten$Size00, S.draws1[[x]],
           mu[x], sqrt(sigsq[x]), beta[x], N[x], n.found1)
  ))})
draws1$W <- exp(draws1$lW - max(draws1$lW))

# Rejection Sampling?
set.seed(623)
draws1$accept <- runif(num.draws1) < draws1$W
```

```{r rejecthist, echo=FALSE, dependson='reject'}
hist(draws1$mu[draws1$accept], freq = FALSE, breaks = 100,
     main = bquote(bold(.(sum(draws1$accept))~'Posterior Draws of'~mu)),
     xlab = expression(mu), ylab = '', yaxt = 'n')
```

But I want POSTERIOR DRAWS!
========================================================

We have a discrete approximation of the posterior.

We can sample with replacement from the draws using the normalized
weights as probabilities.

But most of the distinct values are in the tails! We have little
information about the center of the distribution!

***

```{r resample, echo=FALSE, dependson='reject'}
set.seed(7527)
post.draws1 <- draws1[sample(num.draws1, replace = TRUE, prob = draws1$W),]
hist(post.draws1$mu, freq = FALSE, breaks = 100,
     main = expression(bold('500,000 Resampled Draws of'~mu)),
     xlab = expression(mu), ylab = '', yaxt = 'n')
```

Why not use Gibbs sampling?
========================================================

```{r jagsmodel, echo=TRUE, dependson='reject'}
jags.model <- function(){
  m ~ dcat(p)
  N <- n + m
  mu ~ dnorm(2.38, 1.85)
  sigsq ~ dgamma(3.92, 1.36)
  tau <- pow(sigsq, -1)
  for(i in 1:n){
    y[i] ~ dlnorm(mu, tau)
  }
  for(i in 1:m){
    s[i] ~ dlnorm(mu, tau)
  }
}
```

Why not use Gibbs sampling?
========================================================

```{r jagsfit, eval=FALSE, echo=FALSE, dependson='jagsmodel'}
jags.data <- list('p' = c(rep(0, 12), rep(0.04, 25)),
                  'y' = halten$Size00,
                  'n' = nrow(halten))
jags.params <- c('s', 'N', 'mu', 'sigsq')
jags.out <- jags(data = jags.data, model.file = jags.model,
                 parameters.to.save = jags.params)
```

```{r jagsout, echo=FALSE}
cat('Error in jags.model(model.file, data = data, inits = init.values, n.chains = n.chains,  :
  RUNTIME ERROR:
Compilation error on line 11.
Unknown variable m
Either supply values for this variable with the data
or define it  on the left hand side of a relation.')
```

Not (easily, at least) doable in canned software.

Bespoke Gibbs Sampler
========================================================

Complete conditionals:

$$p(\beta|N,\mu,\sigma^2,\mathbf{S},\mathbf{Y})
\propto p(\beta)\prod_{i=1}^n\left(\frac{Y_i^\beta}
{\sum_{j=i}^nY_j^\beta+\sum_{k=n+1}^NS_k^\beta}\right)$$

$$\begin{aligned}
p(N|\beta,\mu,\sigma^2,\mathbf{S},\mathbf{Y})
&\propto p(N)\prod_{k=n+1}^N\left(\frac{1}{\sqrt{2\pi\sigma^2}S_k}
e^{-\frac{1}{2\sigma^2}(\log S_k-\mu)^2}\right)\\
&\quad\times\begin{pmatrix}N \\ n\end{pmatrix}\prod_{i=1}^n
\left(\frac{1}{\sum_{j=i}^nY_j^\beta+\sum_{k=n+1}^NS_k^\beta}\right)
\end{aligned}$$

Bespoke Gibbs Sampler
========================================================

* Could use priors for $N$ and $\beta$ as proposal for MH
* Normal prior for $\mu$ and inverse-Gamma prior for $\sigma^2$
are conditionally conjugate
* Difficult to program, but doable

Other Issues
========================================================

* Empirical Bayes approach is complicated and unnecessary
* Posteriors summarized by histograms, means, and standard deviations
* Repeated use of ``Bayesian estimator''
* No discussion of a hierarchical version of the model

A Matter of Outreach
========================================================
* Scientists in the field look for easy answers to tough
statistical problems
* Recommending Bayesian methods as a last resort does not
present an accurate picture of Bayesian Statistics
* Bayesian philosophy and techniques should be included
in statistics curricula for the sciences
